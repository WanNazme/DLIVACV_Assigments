{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WanNazme/DLIVACV_Assigments/blob/main/Assessment1_DeepLearning_WanNazme.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpRNh1-L8zuk"
      },
      "source": [
        "## Assessment 1: Deep Learning\n",
        "\n",
        "1) Answer all questions.\n",
        "2) This assessment is open-book. You are allowed to refer to any references including online materials, books, notes, codes, github links, etc.\n",
        "3) Copy this notebook to your google drive (click **FILE** > **save a copy in Drive**)\n",
        "4) Upload the answer notebook to your github. \n",
        "5) Submit the assessment by sharing the link to your answer notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjRauIpz8zun"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "**QUESTION 1** \n",
        "\n",
        "One day while wandering around a clothing store at KL East Mall, you stumbled upon a pretty girl who is choosing a dress for Hari Raya. It turns out that the girl is visually impaired and had a hard time distinguishing between an abaya and a kebaya. To help people with the similar situation, you then decided to develop an AI system to identify the type of clothes using a Convolutional Neural Networks (ConvNet). In order to train the network, you decide to use the Fashion MNIST dataset which is freely available on Pytorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzzvkxpn8zuo"
      },
      "source": [
        "a) Given the problem, what is the most appropriate loss function to use? Justify your answer. **[5 marks]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0hERYSq8zuo"
      },
      "source": [
        "\n",
        "<span style=\"color:blue, font-family: Times New Roman\">\n",
        "    Cross entropy loss function will be the most appropriate to use. It is optimized for multi class classification. It measures the performance of a classification model whose output is a probability value between 0 and 1 which can describe how likely a model is and the error function of each data point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW6A4Pmj8zuo"
      },
      "source": [
        "b) Create and train a ConvNet corresponding to the following CNN architecture (with a modification of the final layer to address the number of classes). Please include **[10 marks]**:\n",
        "\n",
        "    1) The dataloader to load the train and test datasets.\n",
        "\n",
        "    2) The model definition (either using sequential method OR pytorch class method).\n",
        "\n",
        "    3) Define your training loop.\n",
        "\n",
        "    4) Output the mean accuracy for the whole testing dataset.\n",
        "\n",
        "    \n",
        "\n",
        "<div>\n",
        "<img src=\"https://vitalflux.com/wp-content/uploads/2021/11/VGG16-CNN-Architecture.png\" width=\"550\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6pSxtcD2CNNp",
        "outputId": "60657429-a4dc-478f-ed89-0338ab9e33ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting roboflow\n",
            "  Downloading roboflow-0.2.15.tar.gz (18 kB)\n",
            "Collecting certifi==2021.5.30\n",
            "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 9.1 MB/s \n",
            "\u001b[?25hCollecting chardet==4.0.0\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 58.1 MB/s \n",
            "\u001b[?25hCollecting cycler==0.10.0\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.7)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.10)\n",
            "Collecting kiwisolver==1.3.1\n",
            "  Downloading kiwisolver-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 60.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.21.6)\n",
            "Requirement already satisfied: opencv-python-headless>=4.5.1.48 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (7.1.2)\n",
            "Collecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-0.21.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.23.0)\n",
            "Collecting requests_toolbelt\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.15.0)\n",
            "Collecting urllib3==1.26.6\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 69.5 MB/s \n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.64.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (6.0)\n",
            "Collecting requests\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->roboflow) (2.1.1)\n",
            "Building wheels for collected packages: roboflow, wget\n",
            "  Building wheel for roboflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for roboflow: filename=roboflow-0.2.15-py3-none-any.whl size=24936 sha256=3c09f027a0ab231acebcd6660ecafcdd29d7ff1b9d555f00c44a1b55333aecfa\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/e0/a3/fe183fffc3972cb9264e800763d4bbfff80d5309137c410217\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=e14cf02ca553cf44b405d3fee19de561553f760227a5bd351c0274506a7fff7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built roboflow wget\n",
            "Installing collected packages: urllib3, certifi, requests, pyparsing, kiwisolver, cycler, wget, requests-toolbelt, python-dotenv, chardet, roboflow\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.6.15\n",
            "    Uninstalling certifi-2022.6.15:\n",
            "      Successfully uninstalled certifi-2022.6.15\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.4\n",
            "    Uninstalling kiwisolver-1.4.4:\n",
            "      Successfully uninstalled kiwisolver-1.4.4\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.11.0\n",
            "    Uninstalling cycler-0.11.0:\n",
            "      Successfully uninstalled cycler-0.11.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "Successfully installed certifi-2021.5.30 chardet-4.0.0 cycler-0.10.0 kiwisolver-1.3.1 pyparsing-2.4.7 python-dotenv-0.21.0 requests-2.28.1 requests-toolbelt-0.9.1 roboflow-0.2.15 urllib3-1.26.6 wget-3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "cycler",
                  "kiwisolver",
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in Fashion-MNIST-1 to folder: 100% [77847822 / 77847822] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to Fashion-MNIST-1 in folder:: 100%|██████████| 68367/68367 [00:08<00:00, 8308.85it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"UmNQUaFcoH9Ex2TGbrzT\")\n",
        "project = rf.workspace(\"popular-benchmarks\").project(\"fashion-mnist-ztryt\")\n",
        "dataset = project.version(1).download(\"folder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5Ue0OHCL8zup"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "###############################################\n",
        "######## THE REST OF YOUR CODES HERE ##########\n",
        "###############################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dsdDzRtyEepq"
      },
      "outputs": [],
      "source": [
        "# Applying Transforms to the Data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "image_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        # transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        # transforms.RandomRotation(degrees=15),\n",
        "        # transforms.RandomHorizontalFlip(),\n",
        "        # transforms.CenterCrop(size=224),\n",
        "        transforms.Resize(size=200),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        \n",
        "        # transforms.CenterCrop(size=224),\n",
        "        transforms.Resize(size=200),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kB6thr4lmWnh"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /content/Fashion-MNIST-3/test/ankle_boot\n",
        "# !rm -rf /content/Fashion-MNIST-3/test/bag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL1bqOU_Qd-t",
        "outputId": "f98cbeb2-5747-43c1-94f5-f3eec4c826d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "10\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "dataset = '/content/Fashion-MNIST-1'\n",
        "\n",
        "train_directory = os.path.join(dataset, 'train')\n",
        "test_directory = os.path.join(dataset, 'test')\n",
        "\n",
        "# Batch size\n",
        "batchSize = 32\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(os.listdir(train_directory))\n",
        "print(num_classes)\n",
        "\n",
        "# Load Data from folders\n",
        "data = {\n",
        "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
        "\n",
        "    'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'])\n",
        "}\n",
        "\n",
        "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
        "# idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
        "# print(idx_to_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k168cScUZ8r8"
      },
      "outputs": [],
      "source": [
        "# batch_size = 4\n",
        "\n",
        "\n",
        "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "#                                         download=True, transform=transform)\n",
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "#                                           shuffle=True, num_workers=2)\n",
        "\n",
        "# testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "#                                        download=True, transform=transform)\n",
        "# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "#                                          shuffle=False, num_workers=2)\n",
        "\n",
        "# classes = ('coat', 'dress', 'pullover', 'sandal',\n",
        "#        'shirt', 'sneaker', 'trouser', 'tshirt_top')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTuCiNMepGPx",
        "outputId": "624c14db-a323-466e-b60b-d1cf592e92a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58568\n",
            "9775\n"
          ]
        }
      ],
      "source": [
        "#######################################################\n",
        "#                  Create Dataloader                     #\n",
        "#######################################################\n",
        "\n",
        "# Turn train and test custom Dataset's into DataLoader's\n",
        "from torch.utils.data import DataLoader\n",
        "trainloader = DataLoader(dataset=data['train'], # use custom created train Dataset\n",
        "                                     batch_size=batchSize, # how many samples per batch?\n",
        "                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
        "                                     shuffle=True) # shuffle the data?\n",
        "\n",
        "testloader = DataLoader(dataset=data['test'], # use custom created test Dataset\n",
        "                                    batch_size=batchSize, \n",
        "                                    num_workers=0, \n",
        "                                    shuffle=False) # don't usually need to shuffle testing data\n",
        "\n",
        "train_data_size = len(trainloader.dataset)\n",
        "test_data_size = len(testloader.dataset)\n",
        "\n",
        "print(train_data_size)\n",
        "print(test_data_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dCooW-98BU9Q"
      },
      "outputs": [],
      "source": [
        "#1. DEFINE THE CNN \n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 3) \n",
        "        self.conv2 = nn.Conv2d(6, 12, 3) \n",
        "        self.conv3 = nn.Conv2d(12, 24, 3)\n",
        "        self.conv4 = nn.Conv2d(24, 48, 3) \n",
        "        self.conv5 = nn.Conv2d(48, 96, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2) \n",
        "        self.fc1 = nn.Linear(96 * 4 * 4, 32)\n",
        "        self.fc2 = nn.Linear(32, 24)\n",
        "        self.fc3 = nn.Linear(24, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = self.pool(self.relu(self.conv4(x)))\n",
        "        x = self.pool(self.relu(self.conv5(x)))\n",
        "        # print(x.shape)\n",
        "       \n",
        "        x = x.view(-1, 96 * 4 * 4)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfEXt-IrBU9R",
        "outputId": "857906b1-ac5f-45b2-b236-9baae1bd4fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 6, 198, 198]             168\n",
            "              ReLU-2          [-1, 6, 198, 198]               0\n",
            "         MaxPool2d-3            [-1, 6, 99, 99]               0\n",
            "            Conv2d-4           [-1, 12, 97, 97]             660\n",
            "              ReLU-5           [-1, 12, 97, 97]               0\n",
            "         MaxPool2d-6           [-1, 12, 48, 48]               0\n",
            "            Conv2d-7           [-1, 24, 46, 46]           2,616\n",
            "              ReLU-8           [-1, 24, 46, 46]               0\n",
            "         MaxPool2d-9           [-1, 24, 23, 23]               0\n",
            "           Conv2d-10           [-1, 48, 21, 21]          10,416\n",
            "             ReLU-11           [-1, 48, 21, 21]               0\n",
            "        MaxPool2d-12           [-1, 48, 10, 10]               0\n",
            "           Conv2d-13             [-1, 96, 8, 8]          41,568\n",
            "             ReLU-14             [-1, 96, 8, 8]               0\n",
            "        MaxPool2d-15             [-1, 96, 4, 4]               0\n",
            "           Linear-16                   [-1, 32]          49,184\n",
            "             ReLU-17                   [-1, 32]               0\n",
            "           Linear-18                   [-1, 24]             792\n",
            "             ReLU-19                   [-1, 24]               0\n",
            "           Linear-20                   [-1, 10]             250\n",
            "             ReLU-21                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 105,654\n",
            "Trainable params: 105,654\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.46\n",
            "Forward/backward pass size (MB): 7.31\n",
            "Params size (MB): 0.40\n",
            "Estimated Total Size (MB): 8.17\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = CNN() # need to instantiate the network to be used in instance method\n",
        "\n",
        "# 2. LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 3. move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "summary(model, (3,200,200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kV7t8CTzBU9T"
      },
      "outputs": [],
      "source": [
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'cifar10_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHrlnvZuBU9X",
        "outputId": "1b99a55a-c0bd-4113-d68b-52aafe6c9939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 2.3026, Accuracy: 10.7345%, \n",
            "\t\tValidation : Loss : 2.3021, Accuracy: 17.2583%, Time: 138.6203s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 2.2997, Accuracy: 14.7743%, \n",
            "\t\tValidation : Loss : 2.2883, Accuracy: 17.4220%, Time: 135.7381s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 1.7451, Accuracy: 41.6234%, \n",
            "\t\tValidation : Loss : 1.5082, Accuracy: 50.0665%, Time: 135.3597s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 1.2387, Accuracy: 60.5092%, \n",
            "\t\tValidation : Loss : 1.1942, Accuracy: 64.7161%, Time: 135.2843s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 1.0860, Accuracy: 66.7788%, \n",
            "\t\tValidation : Loss : 1.1001, Accuracy: 66.0051%, Time: 134.9679s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.8603, Accuracy: 71.6261%, \n",
            "\t\tValidation : Loss : 0.6463, Accuracy: 74.9668%, Time: 134.7421s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.5776, Accuracy: 77.6328%, \n",
            "\t\tValidation : Loss : 0.6582, Accuracy: 74.8440%, Time: 137.5417s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.5534, Accuracy: 78.4370%, \n",
            "\t\tValidation : Loss : 0.5689, Accuracy: 78.0256%, Time: 134.6781s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.5355, Accuracy: 79.0739%, \n",
            "\t\tValidation : Loss : 0.5591, Accuracy: 78.0665%, Time: 134.1289s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.5211, Accuracy: 79.6032%, \n",
            "\t\tValidation : Loss : 0.5680, Accuracy: 78.0972%, Time: 133.8779s\n"
          ]
        }
      ],
      "source": [
        "# 4. Train the model for 10 epochs\n",
        "\n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-FX-LD4WBU9Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "e997555f-0c4d-4f9e-c8ae-bc78706b84d6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 5. Analyze the loss curve\n",
        "\n",
        "history = np.array(history)\n",
        "plt.plot(history[:,0:2])\n",
        "plt.legend(['Tr Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0,3)\n",
        "# plt.savefig('cifar10_loss_curve.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-2zu_T40BU9Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "20a60aea-94bd-42b4-dad3-efa4e993c234"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 6. Analyze the accuracy curve\n",
        "\n",
        "plt.plot(history[:,2:4])\n",
        "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "# plt.savefig('cifar10_accuracy_curve.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4Wo7eVJ7d1p6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b427b9e7-e533-489c-81c6-8daaec51b853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 78 %\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "#         images, labels = data\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "#         images = images.view(images.size(0), -1)\n",
        "        outputs = model(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1Zjsjxq8zuq"
      },
      "source": [
        "c) Replace your defined CNN in b) with a pre-trained model. Then, proceed with a transfer learning and finetune the model for the Fashion MNIST dataset. **[10 marks]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "44LXAa5bymrG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "03f8f5884b744fa48b04139a7b5ef1ba",
            "36ed8dcafd13492187ca47c1a090bb53",
            "1b6c7f7dde364fd5b731fe289783e369",
            "731eefa8bb784a75a67c9990293ab064",
            "408c5b9833254f71848a3af9378c6530",
            "7573a3d46c0d45c29e462a564d02d058",
            "ad3722658d354d0989b9896274237e9c",
            "5fd38cdbc968457cb128f5f0d9e16daf",
            "e034e1b319024ec1a1978e3b81cbf7c7",
            "b40109975145469d85b7da867e0b86b0",
            "789cd15f43e0475faa73b84091c8ee83"
          ]
        },
        "outputId": "38cb64f5-db48-4054-b4e3-b4b0885dad16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03f8f5884b744fa48b04139a7b5ef1ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model_pm = models.resnet18(pretrained=True)\n",
        "num_ftrs = model_pm.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_pm.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "# 2. LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_pm.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 3. move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model_pm.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775c598f-5500-49a9-efb7-4cae77632d2d",
        "id": "W3HG-rIiCjQO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.3476, Accuracy: 87.8722%, \n",
            "\t\tValidation : Loss : 0.2469, Accuracy: 90.8542%, Time: 253.2013s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.1992, Accuracy: 92.8852%, \n",
            "\t\tValidation : Loss : 0.2009, Accuracy: 93.0639%, Time: 252.4395s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.1567, Accuracy: 94.3997%, \n",
            "\t\tValidation : Loss : 0.1959, Accuracy: 93.0639%, Time: 254.0976s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.1229, Accuracy: 95.5846%, \n",
            "\t\tValidation : Loss : 0.2048, Accuracy: 92.6752%, Time: 252.2935s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.0939, Accuracy: 96.6466%, \n",
            "\t\tValidation : Loss : 0.2040, Accuracy: 93.1969%, Time: 251.7695s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.0710, Accuracy: 97.5208%, \n",
            "\t\tValidation : Loss : 0.2388, Accuracy: 92.9105%, Time: 251.8412s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.0499, Accuracy: 98.3523%, \n",
            "\t\tValidation : Loss : 0.2408, Accuracy: 92.5115%, Time: 251.7140s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.0381, Accuracy: 98.7263%, \n",
            "\t\tValidation : Loss : 0.2249, Accuracy: 93.7903%, Time: 251.7963s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.0271, Accuracy: 99.1019%, \n",
            "\t\tValidation : Loss : 0.2487, Accuracy: 93.5959%, Time: 251.8443s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.0201, Accuracy: 99.3990%, \n",
            "\t\tValidation : Loss : 0.2611, Accuracy: 93.6164%, Time: 253.3277s\n"
          ]
        }
      ],
      "source": [
        "# 4. Train the model for 10 epochs\n",
        "\n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model_pm, criterion, optimizer, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "#         images, labels = data\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "#         images = images.view(images.size(0), -1)\n",
        "        outputs = model_pm(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-0HwG-hRA7K",
        "outputId": "e8db7a78-8926-4f0b-908d-5969410ce966"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 93 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6uCpzFC8zur"
      },
      "source": [
        "d) Using model-centric methods, propose two (2) strategies that can be used to increase the accuracy of the model on the testing dataset. **[5 marks]**\n",
        "\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "    Two model-centric techniques that I propose are: Batch normalization and dropout. </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FIMfUfz8zur"
      },
      "source": [
        "e) Next, implement the two proposed model-centric techniques for the same problem as in the previous question. **[15 marks]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9UIGCk5K8zus"
      },
      "outputs": [],
      "source": [
        "#1. DEFINE THE CNN \n",
        "class CNN_MC(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN_MC, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 3) \n",
        "        self.batchnorm1 = nn.BatchNorm2d(6)\n",
        "        self.conv2 = nn.Conv2d(6, 12, 3) \n",
        "        self.conv3 = nn.Conv2d(12, 24, 3)\n",
        "        self.conv4 = nn.Conv2d(24, 48, 3) \n",
        "        self.conv5 = nn.Conv2d(48, 96, 3)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(96)\n",
        "        self.pool = nn.MaxPool2d(2, 2) \n",
        "        self.fc1 = nn.Linear(96 * 4 * 4, 32)\n",
        "        self.fc2 = nn.Linear(32, 24)\n",
        "        self.fc3 = nn.Linear(24, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.batchnorm1(self.pool(self.relu(self.conv1(x))))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = self.pool(self.relu(self.conv4(x)))\n",
        "        x = self.batchnorm2(self.pool(self.relu(self.conv5(x))))\n",
        "        # print(x.shape)\n",
        "       \n",
        "        x = x.view(-1, 96 * 4 * 4)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(self.relu(self.fc2(x)))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_pm = CNN_MC() # need to instantiate the network to be used in instance method\n",
        "\n",
        "# 2. LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_pm.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 3. move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model_pm.to(device)\n",
        "summary(model_pm, (3,200,200))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvXsPVLJQc8Z",
        "outputId": "b10f809b-dbcb-4231-d64a-6269409a99bf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 6, 198, 198]             168\n",
            "              ReLU-2          [-1, 6, 198, 198]               0\n",
            "         MaxPool2d-3            [-1, 6, 99, 99]               0\n",
            "       BatchNorm2d-4            [-1, 6, 99, 99]              12\n",
            "            Conv2d-5           [-1, 12, 97, 97]             660\n",
            "              ReLU-6           [-1, 12, 97, 97]               0\n",
            "         MaxPool2d-7           [-1, 12, 48, 48]               0\n",
            "            Conv2d-8           [-1, 24, 46, 46]           2,616\n",
            "              ReLU-9           [-1, 24, 46, 46]               0\n",
            "        MaxPool2d-10           [-1, 24, 23, 23]               0\n",
            "           Conv2d-11           [-1, 48, 21, 21]          10,416\n",
            "             ReLU-12           [-1, 48, 21, 21]               0\n",
            "        MaxPool2d-13           [-1, 48, 10, 10]               0\n",
            "           Conv2d-14             [-1, 96, 8, 8]          41,568\n",
            "             ReLU-15             [-1, 96, 8, 8]               0\n",
            "        MaxPool2d-16             [-1, 96, 4, 4]               0\n",
            "      BatchNorm2d-17             [-1, 96, 4, 4]             192\n",
            "           Linear-18                   [-1, 32]          49,184\n",
            "             ReLU-19                   [-1, 32]               0\n",
            "           Linear-20                   [-1, 24]             792\n",
            "             ReLU-21                   [-1, 24]               0\n",
            "          Dropout-22                   [-1, 24]               0\n",
            "           Linear-23                   [-1, 10]             250\n",
            "             ReLU-24                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 105,858\n",
            "Trainable params: 105,858\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.46\n",
            "Forward/backward pass size (MB): 7.77\n",
            "Params size (MB): 0.40\n",
            "Estimated Total Size (MB): 8.63\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train the model for 10 epochs\n",
        "\n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model_pm, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4pExeRgQNYV",
        "outputId": "c5c2a946-2381-4dba-b0aa-c571c8ec5579"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.8136, Accuracy: 72.6011%, \n",
            "\t\tValidation : Loss : 0.4400, Accuracy: 83.3248%, Time: 135.3906s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.4418, Accuracy: 84.5291%, \n",
            "\t\tValidation : Loss : 0.4056, Accuracy: 85.6368%, Time: 135.2256s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.3838, Accuracy: 86.8239%, \n",
            "\t\tValidation : Loss : 0.3296, Accuracy: 88.1023%, Time: 135.5604s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.3523, Accuracy: 87.8483%, \n",
            "\t\tValidation : Loss : 0.2979, Accuracy: 89.4936%, Time: 135.5872s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.3253, Accuracy: 88.7669%, \n",
            "\t\tValidation : Loss : 0.2926, Accuracy: 89.1560%, Time: 135.6251s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.3076, Accuracy: 89.4311%, \n",
            "\t\tValidation : Loss : 0.2846, Accuracy: 89.9130%, Time: 135.2867s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.2886, Accuracy: 90.0065%, \n",
            "\t\tValidation : Loss : 0.2768, Accuracy: 89.9335%, Time: 135.4273s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.2738, Accuracy: 90.5273%, \n",
            "\t\tValidation : Loss : 0.2640, Accuracy: 90.5678%, Time: 135.2810s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.2591, Accuracy: 91.0941%, \n",
            "\t\tValidation : Loss : 0.2849, Accuracy: 90.0358%, Time: 135.1967s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.2439, Accuracy: 91.4919%, \n",
            "\t\tValidation : Loss : 0.2715, Accuracy: 90.4962%, Time: 134.8580s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "#         images, labels = data\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "#         images = images.view(images.size(0), -1)\n",
        "        outputs = model_pm(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "id": "OWftkUhTREkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7946ca1e-3fce-42d5-c131-e19faf9191b5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 90 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzPPxsCX8zus"
      },
      "source": [
        "f) Do you see any accuracy improvement? Whether it is a \"yes\" or \"no\", discuss the possible reasons contributing to the accuracy improvement/ unimprovement. **[5 marks]**\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "    Your answer here </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVArqW8h8zus"
      },
      "source": [
        "g) In real applications, data-centric strategies are essential to train robust deep learning models. Give two (2) examples of such strategies and discuss how the strategies helps improving the model accuracy. **[5 marks]**\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "    Your answer here </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zifLt-s8zut"
      },
      "source": [
        "h) Next, implement the two proposed data-centric techniques for the same problem as in the previous question. **[10 marks]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4KJ2lnCFRk8m"
      },
      "outputs": [],
      "source": [
        "# Applying Transforms to the Data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "image_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=200, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=200),\n",
        "        transforms.Resize(size=200),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=200, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=200),\n",
        "        transforms.Resize(size=200),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train the model for 10 epochs\n",
        "\n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model_pm, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "id": "-eFSsrk1SBPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971c4811-b143-4a66-94a3-b716c17d9986"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.2351, Accuracy: 91.8522%, \n",
            "\t\tValidation : Loss : 0.2735, Accuracy: 90.7315%, Time: 135.0572s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.2188, Accuracy: 92.4276%, \n",
            "\t\tValidation : Loss : 0.2605, Accuracy: 91.0179%, Time: 135.3662s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.2109, Accuracy: 92.7093%, \n",
            "\t\tValidation : Loss : 0.2777, Accuracy: 90.4348%, Time: 135.0979s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.1997, Accuracy: 93.0098%, \n",
            "\t\tValidation : Loss : 0.2716, Accuracy: 90.8031%, Time: 135.3397s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.1923, Accuracy: 93.1584%, \n",
            "\t\tValidation : Loss : 0.2808, Accuracy: 90.7315%, Time: 134.9680s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.1863, Accuracy: 93.3803%, \n",
            "\t\tValidation : Loss : 0.2693, Accuracy: 91.3453%, Time: 135.0433s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.1752, Accuracy: 93.8857%, \n",
            "\t\tValidation : Loss : 0.2811, Accuracy: 91.0691%, Time: 134.9605s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.1654, Accuracy: 94.2221%, \n",
            "\t\tValidation : Loss : 0.2989, Accuracy: 90.8542%, Time: 134.9802s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.1587, Accuracy: 94.4202%, \n",
            "\t\tValidation : Loss : 0.2842, Accuracy: 91.3760%, Time: 134.8918s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.1505, Accuracy: 94.6558%, \n",
            "\t\tValidation : Loss : 0.3217, Accuracy: 90.6087%, Time: 135.2738s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "#         images, labels = data\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "#         images = images.view(images.size(0), -1)\n",
        "        outputs = model_pm(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7946ca1e-3fce-42d5-c131-e19faf9191b5",
        "id": "TZqSrlfmKWCy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 90 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCy3b5888zut"
      },
      "source": [
        "**QUESTION 2** **[35 marks]**\n",
        "\n",
        "Firstly, watch this video:\n",
        "\n",
        "https://drive.google.com/file/d/1bsypahR7I3f_R3DXkfw_tf0BrbCHxE_O/view?usp=sharing\n",
        "\n",
        "This video shows an example of masked face recognition where the deep learning model is able to detect and classify your face even when wearing a face mask. Using the end-to-end object detection pipeline that you have learned, develop your own masked face recognition such that the model should recognize your face even on face mask while recognize other persons as \"others\".\n",
        "\n",
        "Deliverables for this question are:\n",
        "\n",
        "- the model file. Change the name to <your_name>.pt file (e.g. hasan.pt).\n",
        "- a short video (~10 secs) containing your face and your friends faces (for inference)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9oIfLdzS8zut"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03f8f5884b744fa48b04139a7b5ef1ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36ed8dcafd13492187ca47c1a090bb53",
              "IPY_MODEL_1b6c7f7dde364fd5b731fe289783e369",
              "IPY_MODEL_731eefa8bb784a75a67c9990293ab064"
            ],
            "layout": "IPY_MODEL_408c5b9833254f71848a3af9378c6530"
          }
        },
        "36ed8dcafd13492187ca47c1a090bb53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7573a3d46c0d45c29e462a564d02d058",
            "placeholder": "​",
            "style": "IPY_MODEL_ad3722658d354d0989b9896274237e9c",
            "value": "100%"
          }
        },
        "1b6c7f7dde364fd5b731fe289783e369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fd38cdbc968457cb128f5f0d9e16daf",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e034e1b319024ec1a1978e3b81cbf7c7",
            "value": 46830571
          }
        },
        "731eefa8bb784a75a67c9990293ab064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b40109975145469d85b7da867e0b86b0",
            "placeholder": "​",
            "style": "IPY_MODEL_789cd15f43e0475faa73b84091c8ee83",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 115MB/s]"
          }
        },
        "408c5b9833254f71848a3af9378c6530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7573a3d46c0d45c29e462a564d02d058": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad3722658d354d0989b9896274237e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fd38cdbc968457cb128f5f0d9e16daf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e034e1b319024ec1a1978e3b81cbf7c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b40109975145469d85b7da867e0b86b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "789cd15f43e0475faa73b84091c8ee83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}